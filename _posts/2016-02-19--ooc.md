---
layout:     post
title:      Object-oriented complexity theory?
date:       2016-02-19 00:00:00
summary:    This article is about things that I learned from a paper. (Moreover, it demonstrates why this blog needs a (pseudo)code highlighter.)
categories: notes
---



This article is about things that I learned from a paper. (Moreover, it demonstrates why this blog needs a (pseudo)code highlighter.) The paper is [Quadratic Conditional Lower Bounds for String Problems and Dynamic Time Warping](http://arxiv.org/abs/1502.01063), by Karl Bringmann and Marvin Künnemann.

This paper studied problems about alignment of strings, especially strings computing *similarity measures*. Part of their results showed such problems including Longest Common Subsequence(LCS), Edit Distance and Dynamic Time Warping (DTW) cannot be solved in $O(n^{2-\epsilon})​$ for any $\epsilon > 0​$, unless Orthogonal Vector (OV) Conjecture fails (or Strong Exponential Time Hypothesis fails).

## Reduction Techniques

This section is a high-level narration of the paper, so I'm omitting many details. Please refer to the paper about the technical details.

### Similarity measure

On two strings (or sequences) $x$ and $y$, a function $\delta$ on $x,y$ is called a *similarity measure* if it measures the similarity of the two strings. And we want to minimize it. For example, in LCS, the $\delta$ is defined as $\left&#124; x \right&#124; + \left&#124; y \right&#124; - LCS(x,y)$.

### Alignment gadgets

Say we want to find an optimal alignment of two sequences of strings $x_1, \dots, x_n$ and $y_1, \dots, y_n$. We have a function $\delta$, where $\delta(x_i,y_j)$ computes the weight of $x_i$ matching with $y_j$.

We say similarity measure $\delta$ *admits* an *alignment gadget* if given $x_1, \dots, x_n$ and $y_1, \dots, y_n$ any we can construct a two strings $x,y$ (called alignment gadgets) for "general alignment problem" having $\delta$ as its measure.

There are two clever things:

1.  $x_1, \dots, x_n$ and $y_1, \dots, y_n$ can be any strings. So they can be either bits, or strings of a few bits, or even alignment gadgets.
2. $\delta(x,y)$ needs to satisfying some constraints.

The two points above makes it possible to make "nested" alignment gadgets.

### Coordinate values

$\delta$ admits *coordinate values* if there exists "building blocks" $0_X, 0_Y, 1_X, 1_Y$ satisfying

$$\delta(1_X,1_Y) >  \delta(0_X,1_Y) = \delta(1_X,0_Y) = \delta(0_X,0_Y)$$

So they can correspond to zeros and ones in Orthogonal Vectors problem.

### Reduction from Orthogonal Vectors

The paper proved the following theorem:

**We can reduce OV to any problem computing similarity measure admitting an alignment gadget and coordinate values. If this problem can be solved in $O(n^{2-\epsilon})$ time, then OV can be solved in $O(n^{2-\epsilon})$.**

The reduction from OV, in high-level, looks like:

1. Construct "coordinate gadgets": Let $0_X, 0_Y, 1_X, 1_Y$ correspond to zeros and ones in sets $X$ and $Y$.
2. Construct "vector gadgets": concatenate the coordinate gadgets into vector gadgets, by making an alignment gadget from the coordinate gadgets.
3. Construct "normalized vector gadgets": wrap the vector gadgets into normalized vector gadgets, by making an alignment gadget from the vector gadgets.
4. Construct input strings $x, y$: concatenate the normalized vector gadgets, by making an alignment gadget from the normalized vector gadgets.

### Reduction to specific problems

For a specific problem (LCS, Edit Distance, DTW), we just need to

1. Define the similarity function $\delta$.
2. Define the building blocks $0_X, 0_Y, 1_X, 1_Y$.
3. Define a method of concatenating substrings to construct an alignment gadget.
4. (Prove the correctness of the above.)

The constructions are tricky, and proofs are very complicated. (Too bad it's not quite possible to mass-produce reductions.) So please read the paper for details.

After I've summarized the main idea of these reductions, the first thing that came into my mind was — 

## This looks like object-oriented programming

The above reductions is exactly an "object-oriented" thinking. Recall in object-oriented programming languages (take Java for example) there are concepts such as `class`, `interface`, `instance`, etc. (Disclaimer: I *was* sort of familiar with Java long time ago, but now I have forgotten most of it. If you are going to have tests or interviews about this, please don't believe anything in this article.)

Let us think of problems are `class`es, while instances of problems are `instance`s of `class`es. Please note the difference in font: I am using "`class`" for the classes in programming languages, and using "class" for complexity classes.

When they say $\delta$ "admitting an alignment gadget" or "admitting coordinate values", they are defining an `interface`. An `interface` is an abstract `class`, where the data (or properties) and functions (or methods) are declared but not defined. A pseudocode (C-style-pseudo-Java?) would look like this:

```
interface alignment_problem{
  // -------- data --------
    
  // coordinate values (suppose Coordinate is another class)
  Coordinate 0x, 0y, 1x, 1y;
    
  // -------- functions --------
    
  // similarity measure
  abstract number delta(x,y);
    
  // building alignment gadgets
  abstract (string x, string y) alignment_gadget(List of xs, list of ys);
    
  static alignment_problem reduce_from_OV(List of vs){
   //create vector gadgets, taking coordinate gadgets as arguments.
   VG(v) = alignment_gadget( ... )
        
   //create normalized vector gadgets, taking vector gadgets as arguments.
   NVG(v) = alignment_gadget( ... )
        
   //create strings x and y, taking normalized vector gadgets as arguments.
   x = alignment_gadget( ... )
   y = alignment_gadget( ... )
  }
}
```

Note: Because this interface has concrete functions, now I think it's an `abstract class`, rather than an `interface`. But I'd prefer calling it `interface` here, since the word "class" is used and overloaded too much everywhere.

The `alignment_problem` has a function telling us how to build up a reduction from OV, by using the coordinate gadgets as basic elements and calling `alignment_gadget` function as subroutines.

And then, a real problem, say LCS, is a `class` that `implements` the `interface`.

```
class LCS implements alignment_problem{
  0x = 10011;
  0y = 11001;
  1x = 11100;
  1y = 00111;
  
  number delta(x,y)
  {
    return length(x)+length(y)-LCS(x,y);
  }
  
  (string x, string y) alignment_gadget(List of xs, list of ys){
    x = ...
    y = ...
    return (x,y);
  }
  
  static LCS reduce_from_OV(List of vs){
    //Let it inherit the reduce_from_OV function defined in alignment_problem
    ...
    ...
	}
}
```

(Well I have to admit I can hardly write Java now.)

Thus, given an instance of OV, we can reduce from OV to a `class` that `implements` `alignment_problem` using the function `reduce_from_OV`.

## So what?

Why am I amazed by such fact?

Most of the time, when we want to find "something in common for different problems", we look for a class where the problems are members of it, or a more generalized problem that the problems reduce to. (This corresponds to a `reduce_to_...` instead of `reduce_from_...` function in the interface) Thus, if we prove problem $P$ is hard, and problem $P$ is in complexity class $C$, then we consider class $C$ is hard. However it just means class $C$ contains hard problems. If $C$ doesn't have many proved complete problems, then we may suspect most problems in $C$ are not hard.

But here, the "object-oriented" approach avoided this problem. What LCS, Edit Distance and DTW have in common is that they are all sequence alignment problems with similarity measures. This "common property" acts in the middle of reductions between problems. For other similar problems, to show their hardness, we just need to discover they have such property, and try to plug the problem onto this reduction. In this way, every problem having this property is hard.

Thus, one may think about these questions. Can the object-oriented design help us in other reductions? Can we find new `interface`s from more problems, and set up more "`reduce_from_...`" functions?